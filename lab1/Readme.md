# 实验一

> 实验环境：
>
> 操作系统：Windows 10家庭中文版
>
> 编译器：gcc 8.1.0
>
> IDE：Visual Studio Community 2019 16.4.5
>
> CPU：Intel i5-7300HQ 2.50GHz (4核)
>
> 内存：8.00GB

## 实验题目：1.求素数个数

### 算法设计与分析

​		串行求小于n的素数个数的一个朴素方法就是对每个数判断其是否为素数，具体方法则是对于一个数x，拿小于等于$\sqrt x$且不为1的所有正整数去试除x，如果所有的数都不能整除x则它是素数，否则是合数。

​		该串行算法并行化很简单，由于检验的先后顺序没有影响，所以我们可以把所有的要检验的数均匀分给p个线程，分别求出素数个数，最后在主线程求和即可。

​		另外注意到偶数除了2都是合数，并且能很快检验出来，所以分配到检验偶数的线程将很快结束。为了均衡负载提高效率，直接不对偶数进行检验，最后结果+1即可。

### 核心代码

#### MPI版本：

检验素数的函数如下:

```c++
int ispri(int x) {
    if (x == 1) return 0;
    for (int i = 2; i * i <= x; i++)
        if (x % i == 0) return 0;
    return 1;
}
```

主函数中只要统计该线程负责的部分的素数个数，最后用MPI_Reduce加到主线程即可

```c++
for (int i = myid * 2 + 3; i <= n; i += numprocs * 2) {
    cnt += ispri(i);
    //printf("%d\n",i);
}
MPI_Reduce(&cnt, &tot, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
```

#### OpenMP版本：

基本上和MPI版类似，并行部分各自统计素数个数，结束并行阶段后求和即可：

```c++
#pragma omp parallel 
{
	double x;
	int id;
	id = omp_get_thread_num();
	sum[id] = 0;
	for (int i = id *2+3; i < num_steps; i = i + NUM_THREADS*2) {
		bool flag = 1;
		for (int j = 2; j < i; j++) {
			if (i % j == 0) {
				flag = 0;
				break;
			}
		}
		if (flag) sum[id]++;
	}
}
for (int i = 0; i < NUM_THREADS; i++) {
	tot += sum[i];
	//printf("%d %d\n", i, sum[i]);
}
```

### 实验结果

规模大小由检验范围n决定，运行用时单位为秒

#### MPI版本：

n=1000,进程数=4时的运行结果：

<img src="E:\STUDY\Parallel Computing\lab1\4.jpg" style="zoom:67%;" />

​																						运行时间

| 规模\进程数 | 1        | 2        | 4        | 8        |
| ----------- | -------- | -------- | -------- | -------- |
| 1000        | 0.000112 | 0.000171 | 0.000473 | 0.000773 |
| 10000       | 0.000789 | 0.000508 | 0.000620 | 0.000773 |
| 100000      | 0.012003 | 0.006596 | 0.004824 | 0.005554 |
| 500000      | 0.105075 | 0.057229 | 0.030427 | 0.031213 |

​																						加速比

| 规模\进程数 | 1    | 2     | 4     | 8     |
| ----------- | ---- | ----- | ----- | ----- |
| 1000        | 1    | 0.655 | 0.237 | 0.145 |
| 10000       | 1    | 1.553 | 1.273 | 1.021 |
| 100000      | 1    | 1.820 | 2.488 | 2.161 |
| 500000      | 1    | 1.836 | 3.453 | 3.366 |

#### OpenMP版本

n=1000，进程数=4时的运行结果如下图:

<img src="E:\STUDY\Parallel Computing\lab1\1.jpg" style="zoom:67%;" />

​																						运行时间

| 规模\进程数 | 1        | 2        | 4        | 8        |
| ----------- | -------- | -------- | -------- | -------- |
| 1000        | 0.000410 | 0.000422 | 0.000447 | 0.001367 |
| 10000       | 0.019493 | 0.010215 | 0.008287 | 0.008405 |
| 100000      | 1.671153 | 0.904029 | 0.467713 | 0.445340 |
| 500000      | 34.54575 | 16.66909 | 9.572410 | 9.242283 |

​																						加速比

| 规模\进程数 | 1    | 2     | 4     | 8     |
| ----------- | ---- | ----- | ----- | ----- |
| 1000        | 1    | 0.972 | 0.917 | 0.300 |
| 10000       | 1    | 1.908 | 2.352 | 2.319 |
| 100000      | 1    | 1.849 | 3.573 | 3.752 |
| 500000      | 1    | 2.072 | 3.609 | 3.738 |

### 数据分析

从上面的数据可以看出，当数据规模很小时，并行的效果并不好。因为生成线程也需要消耗资源和时间，最后的结果并不一定优于直接使用串行程序。而当数据规模逐渐增大后，并行的效果将逐渐显现出来，加速比将趋近于线程数。由于本机实际核心数为4，所以8线程并不能带来比4线程更优的结果，这启示我们线程数需要根据实际的核心数来决定。

## 实验题目：2.求$\pi$

### 算法设计与分析

圆周率$\pi$可以用以下积分式来计算：
$$
\pi=\int_0^1\frac{4}{1+x^2}dx
$$
问题便转化为了求数值积分。串行的积分方法就是把积分区间划分成很多小块，用端点的函数值乘以区间长度近似作为区间的积分值，然后把所有区间的近似积分值相加就是$\pi$的近似值了。

由于数值积分的计算顺序没有影响，并行化十分容易，只需要把每个小积分区间均匀分给每个进程即可。最后主进程对所有进程的结果求和即可。

### 核心代码

#### MPI版本：

每个线程计算一部分的积分值，最后在主进程求和即可

```c++
h = 1.0 / n;
sum = 0;
for (int i = myid + 1; i <= n; i += numprocs) {
    x = h * (i - 0.5);
    sum += 4.0 / (1 + x * x);
}
sum *= h;
MPI_Reduce(&sum,&ans_pi,1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
```

#### OpenMP版本：

实现思路基本类似，换成OpenMP的语法即可

```c++
#pragma omp parallel 
{
	double x;
	int id;
	id = omp_get_thread_num();
	sum[id] = 0;
	for (int i = id; i < num_steps; i = i + NUM_THREADS) {
		x = (i + 0.5) * step;
		sum[id] += 4.0 / (1.0 + x * x);
	}
}

for (int i = 0; i < NUM_THREADS; i++) {
	tot += sum[i] * step;
}
```

### 实验结果

规模大小由数值积分的矩形个数决定，运行用时单位为秒

#### MPI版本：

n=1000,线程数为4时结果如下：

<img src="E:\STUDY\Parallel Computing\lab1\2.jpg" style="zoom:67%;" />

​																						运行时间

| 规模\进程数 | 1        | 2        | 4        | 8        |
| ----------- | -------- | -------- | -------- | -------- |
| 1000        | 0.000060 | 0.000155 | 0.000287 | 0.000533 |
| 10000       | 0.000191 | 0.000164 | 0.000442 | 0.000794 |
| 50000       | 0.00775  | 0.000563 | 0.000825 | 0.000883 |
| 100000      | 0.001746 | 0.000980 | 0.001165 | 0.001326 |
| 500000000   | 7.610930 | 3.762196 | 1.951838 | 1.937120 |

​																						加速比

| 规模\进程数 | 1    | 2     | 4     | 8     |
| ----------- | ---- | ----- | ----- | ----- |
| 1000        | 1    | \     | \     | \     |
| 10000       | 1    | \     | \     | \     |
| 50000       | 1    | \     | \     | \     |
| 100000      | 1    | \     | \     | \     |
| 500000000   | 1    | 2.023 | 3.899 | 3.929 |

#### OpenMP版本

n=200000000,进程数=8时结果如下：

<img src="E:\STUDY\Parallel Computing\lab1\3.jpg" style="zoom:67%;" />

​																						运行时间

| 规模\进程数 | 1        | 2        | 4        | 8        |
| ----------- | -------- | -------- | -------- | -------- |
| 1000        | 0.000214 | 0.000357 | 0.000479 | 0.000639 |
| 10000       | 0.000399 | 0.000468 | 0.000921 | 0.005904 |
| 50000       | 0.001479 | 0.001270 | 0.001343 | 0.000874 |
| 100000      | 0.002440 | 0.002181 | 0.003518 | 0.001195 |
| 200000000   | 3.116278 | 1.578220 | 1.344383 | 1.739998 |

​																							加速比

| 规模\进程数 | 1    | 2     | 4     | 8     |
| ----------- | ---- | ----- | ----- | ----- |
| 1000        | 1    | \     | \     | \     |
| 10000       | 1    | \     | \     | \     |
| 50000       | 1    | \     | \     | \     |
| 100000      | 1    | \     | \     | \     |
| 200000000   | 1    | 1.975 | 2.378 | 1.791 |

### 数据分析

从实验的数据上看，由于实验数据规模仍然不够大，程序运行用时很短，甚至小于实验误差，此时的加速比意义不大，使用并行的方法并不能带来更高的效率，反而由于要创建进程消耗了更多时间。当规模增大后并行的效果才开始显现。

## 实验总结

通过此次实验，我初步掌握了使用OpenMP与MPI编写并行程序的方法，并分别使用了这两种方法编写了简单的并行程序，还了解并实践了使用加速比衡量并行程序性能的评估方法。